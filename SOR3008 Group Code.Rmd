---
title: "SOR3008"
author: "Shane Gallagher"
date: "2025-03-10"
output: html_document
---
libraries
```{r}
##Import library
library(readr)  
library(dplyr)
library(rpart)
if (!requireNamespace("rpart.plot", quietly = TRUE)) {
  install.packages("rpart.plot")
}
library(rpart.plot)
if (!requireNamespace("ROCR", quietly = TRUE)) {
  install.packages("ROCR")
}
library(ROCR)
```

Data processing/cleaning
```{r}
##Import the datasets
obesity_nonbinary <- readr::read_csv('ObesityDataSet.csv')

##Filter and classify overdue Obesity 
## obese = 1, not obese = 0
obesity <- obesity_nonbinary %>%
  mutate(Obesity = ifelse(NObeyesdad %in% c("Obesity_Type_I", "Obesity_Type_II", "Obesity_Type_III"), 1, 0)) %>%
  select(-NObeyesdad) %>%
  dplyr::mutate(
    Gender = ifelse(Gender == "Male", 0, 1),  ##Make gender binary (M=0, F=1)
    family_history_with_overweight = ifelse(family_history_with_overweight == "no", 0, 1),  ##(N=0, Y=1)
    FAVC = ifelse(FAVC == "no", 0, 1),  ##(N=0, Y=1)
    SCC = ifelse(SCC == "no", 0, 1), ##(n=0, y=1)
    CAEC = as.factor(CAEC),
    SMOKE = ifelse(SMOKE == "no", 0, 1),
    CALC = as.factor(CALC),
    MTRANS = as.factor(MTRANS)
    )

# Check the distribution of obesity
prop.table(table(obesity$Obesity))
```

```{r}
library(ggplot2)
if (!requireNamespace("DataExplorer", quietly = TRUE)) {
  install.packages("DataExplorer")
}
library(DataExplorer)
if (!requireNamespace("corrplot", quietly = TRUE)) {
  install.packages("corrplot")
}
library(corrplot)

##Check missing values
missing_summary <- colSums(is.na(obesity))
missing_summary[missing_summary > 0]  # Display only columns with missing values

##Visualising missingness
plot_missing(obesity)

##Select only numeric columns
numeric_features <- obesity %>%
  select_if(is.numeric)

##Compute correlation matrix
cor_matrix <- cor(numeric_features)

##Visualize correlations
corrplot(cor_matrix, method = "color", tl.cex = 0.7, type = "lower")
```
Creation of the test, train and validation datasets

```{r}
##No of observations in data
nobs <- nrow(obesity)

seed<-3008
training_prop<-0.7
validation_prop<-0.1

##Obtain the training rows
set.seed(seed)
train <- sample(nobs, training_prop*nobs)

##Obtain the validation rows
set.seed(seed)
nobs %>%
  seq_len() %>%
  setdiff(train) %>%
  sample(validation_prop*nobs) ->
  validate

##Obtain the test rows
nobs %>%
  seq_len() %>%
  setdiff(train) %>%
  setdiff(validate) ->
  test

##These are the inputs and target appropriate for the neuro
inputs <- c("Gender", "Age", "Height", "Weight", "family_history_with_overweight", "FAVC", "FCVC","NCP","CAEC","SMOKE","CH2O","SCC","FAF","TUE","CALC","MTRANS")
target <- "Obesity"

##Obtain the training, validation and test sets based on the rows identified by train/validate/test
data_train <- obesity[train, c(inputs, target)]
nobs_train <-nrow(data_train)
data_validate <- obesity[validate, c(inputs, target)]
data_test <- obesity[test, c(inputs, target)]

##Export training, validation and test set in case you need in another program e.g. SAS
write.csv(data_train,'data_train.csv')
write.csv(data_validate,'data_validate.csv')
write.csv(data_test,'data_test.csv')
write.csv(obesity, 'obesity.csv')
```

Classification tree
```{r}
##Build a decision tree model using the correct dataset and variables
#ToPredict <- data_train[,c(target)]

set.seed(seed)
rpart_results <- rpart(Obesity ~ .,
                       data = data_train, ##Use the training dataset
                       method = "class",
                       parms = list(split="information"),
                       control = rpart.control(minsplit = 20, minbucket=10, cp = 0,
                                               xval=10,
                                               maxsurrogate = 0),
                       model=TRUE)

##Plot the decision tree
plot1 <- rpart.plot(rpart_results, digits = 3, cex = 1, faclen = 0)
png("decision_tree.png", width = 1000, height = 600)  
rpart.plot(rpart_results, digits = 3, cex = 1, faclen = 0)
dev.off()  

printcp(rpart_results)

png("cp_plot.png", width = 1000, height = 600) 
plotcp <- plotcp(rpart_results, minline=TRUE,lty=3, col=1, upper="splits")
dev.off()  # Save and close the file



##Prune tree based on complexity parameter
pt<-prune(rpart_results, cp= 0.01)
plot2 <- rpart.plot(pt, digits = 3, cex=0.6, faclen=0)
png("pruned_decision_tree.png", width = 1000, height = 600) 
rpart.plot(pt, digits = 3, cex=1, faclen=0)
dev.off()  

##Predict probabilities on the training data
pred_probs <- predict(pt, newdata = data_train, type = "prob")

##Assign predicted probabilities
pt$pred_obese <- pred_probs[,2]
pt$pred_notobese <- pred_probs[,1]

##Calculate AUC for training set
pred <- prediction(pt$pred_obese, data_train$Obesity)
prf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(prf)

png("auc_training.png", width = 1000, height = 600) 
plot(performance(pred, measure = "tpr", x.measure = "fpr"))
dev.off()  # Save and close the file

auc <- performance(pred, measure = "auc")
auc_value <- auc@y.values[[1]]
print(paste("Training AUC value:", auc_value))

##Generate classification results
pt$fitted_results <- ifelse(pt$pred_obese >= 0.5, 1, 0)
print(table(pt$fitted_results, data_train$Obesity))

resubClassificationError <- mean(pt$fitted_results != data_train$Obesity)
print(paste("Re-substituition Error rate:", resubClassificationError))

##Predictions on data test set

pred_probs_test <- predict(pt, newdata = data_test, type = "prob")

# Assign predicted probabilities
pt$pred_obese <- pred_probs_test[,2]
pt$pred_notobese <- pred_probs_test[,1]

# Calculate AUC for test set
predtest <- prediction(pt$pred_obese, data_test$Obesity)
prftest <- performance(predtest, measure = "tpr", x.measure = "fpr")
plot(prftest)

png("auc_test.png", width = 1000, height = 600) 
plot(performance(predtest, measure = "tpr", x.measure = "fpr"))
dev.off()  # Save and close the file

auc_test <- performance(predtest, measure = "auc")
auc_test_value <- auc_test@y.values[[1]]
print(paste("Test AUC value:", auc_test_value))

pt$fitted_results_test <- ifelse(pt$pred_obese >= 0.5, 1,0)
conf_matrix <- table(pt$fitted_results_test, data_test$Obesity)
conf_matrix

misClassificError <- mean(pt$fitted_results_test != data_test$Obesity)
misClassificError
predaccuracy <- 1 - misClassificError
predaccuracy

print(paste("Predictive accuracy:", predaccuracy))
print(paste("Misclassification error:", misClassificError))

##Extracting values from confusion matrix
TP <- conf_matrix[2, 2]  ##True Positives
TN <- conf_matrix[1, 1]  ##True Negatives
FP <- conf_matrix[1, 2]  ##False Positives
FN <- conf_matrix[2, 1]  ##False Negatives

##Accuracy
accuracy <- (TP + TN) / sum(conf_matrix)

##Precision
precision <- TP / (TP + FP)

##Recall (Sensitivity)
recall <- TP / (TP + FN)

##Print metrics
cat("Accuracy: ", accuracy, "\n")
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")
```


Extreme Gradient Boosting

```{r}
library(xgboost)
library(caTools)
library(dplyr)

y_train <- as.integer(data_train$Obesity)
y_test <- as.integer(data_test$Obesity)
X_train <- data_train %>% select(-Obesity)
X_train <- model.matrix(~ . + 0, data = X_train)
X_test <- data_test %>% select(-Obesity)
X_test <- model.matrix(~ . + 0, data = X_test)

xgb_train <-xgb.DMatrix(data=as.matrix(X_train), label = as.numeric(y_train))
xgb_test <- xgb.DMatrix(data=as.matrix(X_test), label = y_test)
xgb_params <- list(
  booster = "gbtree",
  eta = 0.01,
  max_depth = 8,
  gamma = 4, 
  subsample = 0.75,
  colsample_bytree = 1,
  objective = "binary:logistic",
  eval_metric = "logloss"
)

xgb_model <- xgb.train(
  params = xgb_params,
  data = xgb_train,
  nrounds = 1000, 
  verbose = 1
)

xgb_model


xgb_preds_prob_train <- predict(xgb_model, as.matrix(X_train))  # Predicted probabilities
xgb_preds_train <- ifelse(xgb_preds_prob_train > 0.5, 1, 0)

xgb_preds_df_train <- data.frame(
  PredictedClass = as.factor(xgb_preds_train),
  ActualClass = as.factor(y_train)
)

##Calculate accuracy
accuracy <- sum(xgb_preds_df_train$PredictedClass == xgb_preds_df_train$ActualClass) / nrow(xgb_preds_df_train)
print(paste("Accuracy:", accuracy))

##Create confusion matrix
conf_matrix_train <- table(Predicted = xgb_preds_df_train$PredictedClass, Actual = xgb_preds_df_train$ActualClass)
conf_matrix_train

##Extracting values from confusion matrix
TP <- conf_matrix_train[2, 2]  
TN <- conf_matrix_train[1, 1]  
FP <- conf_matrix_train[1, 2]  
FN <- conf_matrix_train[2, 1]  

##Accuracy
accuracy <- (TP + TN) / sum(conf_matrix_train)

##Precision
precision <- TP / (TP + FP)

##Recall
recall <- TP / (TP + FN)

##Print metrics
cat("Accuracy: ", accuracy, "\n")
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")

png("importance_matrix.png", width = 1000, height = 600) 
importance_matrix <- xgb.importance(feature_names = colnames(X_train), model = xgb_model)
dev.off() 

##Plot feature importance
xgb.plot.importance(importance_matrix)

predtrain <- prediction(xgb_preds_prob_train, y_train)

prftrain <- performance(predtrain, measure = "tpr", x.measure = "fpr")

##Plot the ROC curve
png("auc_train_xgb.png", width = 1000, height = 600) 
plot(prftrain, main = "ROC Curve")
dev.off()  

##Calculate AUC (Area Under Curve)
auc_train <- performance(predtrain, measure = "auc")
auc_train_value <- auc_train@y.values[[1]]

##Print the AUC value
print(paste("AUC Value: ", auc_train_value))


##Resubstitution Error (Error on training set)
resubstitution_error <- mean(xgb_preds_train != data_train$Obesity)
cat("Resubstitution Error:", resubstitution_error, "\n")

##Test set

xgb_preds_prob <- predict(xgb_model, as.matrix(X_test))  
xgb_preds <- ifelse(xgb_preds_prob > 0.5, 1, 0)

xgb_preds_df <- data.frame(
  PredictedClass = as.factor(xgb_preds),
  ActualClass = as.factor(y_test)
)

##Calculate accuracy
accuracy <- sum(xgb_preds_df$PredictedClass == xgb_preds_df$ActualClass) / nrow(xgb_preds_df)
print(paste("Accuracy:", accuracy))

##Create confusion matrix
conf_matrix <- table(Predicted = xgb_preds_df$PredictedClass, Actual = xgb_preds_df$ActualClass)
conf_matrix

##Extracting values from confusion matrix
TP <- conf_matrix[2, 2]  # True Positives
TN <- conf_matrix[1, 1]  # True Negatives
FP <- conf_matrix[1, 2]  # False Positives
FN <- conf_matrix[2, 1]  # False Negatives

##Accuracy
accuracy <- (TP + TN) / sum(conf_matrix)

##Precision
precision <- TP / (TP + FP)

##Recall
recall <- TP / (TP + FN)

##Print metrics
cat("Accuracy: ", accuracy, "\n")
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")

importance_matrix <- xgb.importance(feature_names = colnames(X_train), model = xgb_model)

##Plot feature importance

xgb.plot.importance(importance_matrix)

predtest <- prediction(xgb_preds_prob, y_test)

##Generate performance for True Positive Rate (TPR) vs. False Positive Rate (FPR)
prftest <- performance(predtest, measure = "tpr", x.measure = "fpr")

##Plot the ROC curve
png("auc_test_xgb.png", width = 1000, height = 600) 
plot(prftest, main = "ROC Curve")
dev.off()  


##Calculate AUC (Area Under Curve)
auc_test <- performance(predtest, measure = "auc")
auc_test_value <- auc_test@y.values[[1]]

print(paste("AUC Value: ", auc_test_value))

```
SVM MODEL
```{r}
library(tidyverse)
library(e1071)
library(pROC)

##Ensure 'Obesity' is a factor for classification
data_train$Obesity <- as.factor(data_train$Obesity)
data_test$Obesity <- as.factor(data_test$Obesity)

##Convert categorical variables to dummy variables (excluding target variable)
train_data <- model.matrix(Obesity ~ . - 1, data = data_train)  # Removes intercept
test_data <- model.matrix(Obesity ~ . - 1, data = data_test)

##Convert to data frames
train_data <- as.data.frame(train_data)
test_data <- as.data.frame(test_data)

train_data$Obesity <- data_train$Obesity
test_data$Obesity <- data_test$Obesity

num_features <- sapply(train_data, is.numeric)

##Scale numeric features (excluding 'Obesity')
train_data[, num_features] <- scale(train_data[, num_features])
test_data[, num_features] <- scale(test_data[, num_features])

svm_model <- svm(Obesity ~ ., data = data_train, 
                 type = "C-classification", 
                 kernel = "radial")

summary(svm_model)

##Predict on both training and test data
svm_pred_train <- predict(svm_model, data_train)
svm_pred_test <- predict(svm_model, data_test)

##Train data
##Confusion matrix for test data
conf_matrix_train <- table(Predicted = svm_pred_train, Actual = data_train$Obesity)
print("Test Confusion Matrix:")
print(conf_matrix_train)

##Resubstitution Error (Error on training set)
resubstitution_error <- mean(svm_pred_train != data_train$Obesity)
cat("Resubstitution Error:", resubstitution_error, "\n")

# Predictive Accuracy (on train data)
accuracy_train <- mean(svm_pred_train == data_train$Obesity)
cat("Predictive Accuracy on Train Data:", accuracy_train, "\n")

##Extracting values from confusion matrix
TP <- conf_matrix_train[2, 2]  
TN <- conf_matrix_train[1, 1]  
FP <- conf_matrix_train[1, 2]  
FN <- conf_matrix_train[2, 1]  

##Accuracy
accuracy <- (TP + TN) / sum(conf_matrix_train)

##Precision
precision <- TP / (TP + FP)

##Recall
recall <- TP / (TP + FN)

##print metrics
cat("Accuracy: ", accuracy, "\n")
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")


# Calculate AUC (Area Under ROC Curve)
roc_curve <- roc(data_train$Obesity, as.numeric(svm_pred_train))
png("auc_train_svm.png", width = 1000, height = 600) 
roc_curve
dev.off()  # Save and close the file
auc_value <- auc(roc_curve)
cat("AUC (Area Under Curve):", auc_value, "\n")


## Test data
##Confusion matrix for test data
conf_matrix_test <- table(Predicted = svm_pred_test, Actual = data_test$Obesity)
print("Test Confusion Matrix:")
conf_matrix_test

##Predictive Accuracy (on test data)
accuracy_test <- mean(svm_pred_test == data_test$Obesity)
cat("Predictive Accuracy on Test Data:", accuracy_test, "\n")

##Extracting values from confusion matrix
TP_t <- conf_matrix_test[2, 2]
TN_t <- conf_matrix_test[1, 1]  
FP_t <- conf_matrix_test[1, 2]  
FN_t <- conf_matrix_test[2, 1]  

##Accuracy
accuracy <- (TP_t + TN_t) / sum(conf_matrix_test)

##Precision
precision <- TP_t / (TP_t + FP_t)

##Recall 
recall <- TP_t / (TP_t + FN_t)

##Print metrics
cat("Accuracy: ", accuracy, "\n")
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")

##Calculate AUC (Area Under ROC Curve)
roc_curve <- roc(data_test$Obesity, as.numeric(svm_pred_test))
png("auc_test_svm.png", width = 1000, height = 600) 
roc_curve
dev.off()  # Save and close the file
auc_value <- auc(roc_curve)
cat("AUC (Area Under Curve):", auc_value, "\n")

```

LOGISTIC REGRESSION

```{r}
if (!requireNamespace("lme4", quietly = TRUE)) {
  install.packages("lme4")
}
library(lme4)

res_logistic <- glm(Obesity ~ family_history_with_overweight + Age 
                    +Gender + FAVC + FCVC + NCP + CAEC 
                    + SMOKE + CH2O + SCC + FAF + TUE + CALC
                    + MTRANS ,data=data_train, family=binomial())
##removed height cause greatest vif
##removed weight cause high corr with obesity


summary(res_logistic)
exp(cbind(OR=coef(res_logistic),
confint.default(res_logistic)))

data_train$predobese <- predict(res_logistic, newdata = data_train,
type = "response")
data_train$prednotobese<-1-data_train$predobese
#If 50% probability cut-off
data_train$fitted.results <- ifelse(data_train$prednotobese >
0.5,1,0)

resub_Errorrate <- mean(data_train$fitted.results != data_train$Obesity)
resub_Errorrate
table(data_train$Obesity, data_train$fitted.results)

pr <- prediction(data_train$predobese, data_train$Obesity)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc

#Package installation

remove.packages("cli")

install.packages("cli")

install.packages("broom")
install.packages("pbkrtest")
remove.packages("Matrix")

install.packages("Matrix", type="source")


install.packages("car")

library(car)


"car" %in% rownames(installed.packages())

car::vif(res_logistic)

summary(res_logistic)

res_logistic_step <- step(res_logistic,direction="forward",trace=TRUE)

summary(res_logistic_step)


##TEST DATA

data_test$predobese <- predict(res_logistic, newdata = data_test,
type = "response")
data_test$prednotobese<-1-data_test$predobese
#If 50% probability cut-off
data_test$fitted.results <- ifelse(data_test$prednotobese >
0.5,0,1)

resub_Errorrate <- mean(data_test$fitted.results != data_test$Obesity)
resub_Errorrate
conf_matrix_log <- table(data_test$Obesity, data_test$fitted.results)
conf_matrix_log

##Extracting values from confusion matrix
TP <- conf_matrix_log[2, 2]  # True Positives
TN <- conf_matrix_log[1, 1]  # True Negatives
FP <- conf_matrix_log[1, 2]  # False Positives
FN <- conf_matrix_log[2, 1]  # False Negatives

##Accuracy
accuracy <- (TP + TN) / sum(conf_matrix_log)

##Precision
precision <- TP / (TP + FP)

##Recall
recall <- TP / (TP + FN)

##Print metrics
cat("Accuracy: ", accuracy, "\n")
cat("Precision: ", precision, "\n")
cat("Recall: ", recall, "\n")

pr <- prediction(data_test$predobese, data_test$Obesity)

prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc

# Calculate resubstitution accuracy
resub_accuracy_log <- sum(diag(conf_matrix_log)) / sum(conf_matrix_log)

# Resubstitution error is 1 - accuracy
resub_error_log <- 1 - resub_accuracy_log

print(paste("Resubstitution Error:", resub_error_log))



NAIVE BAYES 

```{r}
library(e1071)
library(pROC)

# Train Naive Bayes model
nb_model <- naiveBayes(Obesity ~ ., data=data_train)

nb_model

# Make predictions
nb_pred <- predict(nb_model, newdata=data_test, type="class")
nb_prob <- predict(nb_model, newdata=data_test, type="raw")

print(nb_pred)
print(nb_prob)

# Create confusion matrix
confusion_matrix_nb <- table(Actual = data_test$Obesity, Predicted = nb_pred)
print(confusion_matrix_nb)

TP <- confusion_matrix_nb[2, 2]
FP <- confusion_matrix_nb[1, 2]
FN <- confusion_matrix_nb[2,1]
TN <- confusion_matrix_nb[1,1]


# Accuracy
accuracy_nb <- sum(diag(confusion_matrix_nb)) / sum(confusion_matrix_nb)
print(paste("Accuracy: ", accuracy_nb))

#Precision
precision_nb <- TP / (TP + FP)
print(paste("Precision:", precision_nb))

#Recall
Recall_nb <- TP / (TP + FN)
print(paste("Recall:", Recall_nb))

#F1-Score
F1_score <- 2 * (precision * Recall) / (precision + Recall)
print(paste("F1 score:", F1_score))

# Log-Likelihood Calculation
log_likelihood <- sum(log(nb_prob[cbind(1:nrow(nb_prob), as.numeric(data_test$Obesity))]))
print(paste("Log-Likelihood: ", log_likelihood))

# ROC Curve
roc_curve_nb <- roc(data_test$Obesity, nb_prob[,2])  # assuming Obesity_NUM is binary for 1 vs 0
plot(roc_curve_nb, main="ROC Curve for Naive Bayes")

auc_value_nb <- auc(roc_curve_nb)

print(paste("AUC:", auc_value_nb))

# Calculate resubstitution accuracy
resub_accuracy_nb <- sum(diag(confusion_matrix_nb)) / sum(confusion_matrix_nb)

# Resubstitution error is 1 - accuracy
resub_error_nb <- 1 - resub_accuracy_nb

print(paste("Resubstitution Error:", resub_error_nb))
```

